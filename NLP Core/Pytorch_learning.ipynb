{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "聊到深度学习, 大家第一感觉就是很高大上。\n",
    "就像我们曾经说到机器学习，很多人也是感觉很高大上，但是慢慢接触之后，发现其无非是数学+编程实现，所以从线性回归开始，不断学习，把各种机器学习方法都学习了一遍，并能够通过Python的sklearn库编程实现。\n",
    "有很多朋友和我聊到学习深度学习这个事情，我会推荐他们去看一些相关理论算法，从CNN、RNN到LSTM，从各种传统的深度学习网络结构到比较新的网络结构。在这个过程中发现了一点，就是他们看完了网络结构和算法原理后，往往难以下手去操作，因为Python使用sklearn库实现机器学习的流程非常方便，而且学习了Python的numpy、pandas基础即可快速上手，而深度学习的编程则需要更上一层，对Python新手不太友好，基础部分的张量各种操作和计算图编程都会让新手头大，而深度学习还面临着选择深度学习框架的问题，可供我们选择的框架实在太多，例如较为流行的Tensorflow、Pytorch、Keras、Mxnet、PaddlePaddle等等。各种相关的书也是如雨后春笋般出版出来，我翻看过一些书，还看过一些目录，发现了一个现象，就是各种深度学习框架书在对应的深度学习框架基础部分讲解往往不够详细，让人看完觉得跳跃感很大，而过于强调各种网络结构的实现，这十分不利于新手进行学习。而且初学者会了解到深度学习需要使用GPU训练模型，对于没有GPU配置的初学者，往往觉得学习之路出现了第一道坎，而对于有GPU的初学者，往往因为GPU环境搭建太复杂而从入门到放弃。其实没有GPU环境，也是可以进行深度学习编程学习的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型为： float64\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "arr = np.ones((3,3))\n",
    "print(\"数据类型为：\",arr.dtype)\n",
    "t = torch.tensor(arr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "t = torch.from_numpy(arr)\n",
    "print(arr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[999   2   3]\n",
      " [  4   5   6]]\n",
      "tensor([[999,   2,   3],\n",
      "        [  4,   5,   6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "t = torch.from_numpy(arr)\n",
    "\n",
    "#修改array内容\n",
    "arr[0,0] = 999\n",
    "\n",
    "print(arr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      " tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "2084605375816 2084605375816 True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1,2,3])#创建一个t1张量，赋一个初始值\n",
    "t = torch.zeros((2,3),out=t1)#将创建的t张量输出到t1\n",
    "print(t,'\\n',t1)\n",
    "print(id(t),id(t1),id(t)==id(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.empty(3, 5)\n",
    "t = torch.zeros_like(input)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 8.],\n",
      "        [8., 8.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.full((2,2),8)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 8., 8., 8., 8.],\n",
      "        [8., 8., 8., 8., 8.],\n",
      "        [8., 8., 8., 8., 8.]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.empty(3, 5)\n",
    "t = torch.full_like(input,8)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(1,9,2)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 2.3333, 3.6667, 5.0000, 6.3333, 7.6667, 9.0000])\n"
     ]
    }
   ],
   "source": [
    "t = torch.linspace(1,9,7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10])\n"
     ]
    }
   ],
   "source": [
    "t = torch.logspace(start=-5, end=10,steps=4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.eye(4)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成正态分布数据的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:tensor([1., 2., 3., 4., 5.]),std:tensor([1., 2., 3., 4., 5.])\n",
      "tensor([ 1.5956, -0.9349,  1.7945,  8.0524,  7.9456])\n"
     ]
    }
   ],
   "source": [
    "#mean为张量，std为张量\n",
    "mean = torch.arange(1,6,dtype=torch.float)\n",
    "std = torch.arange(1,6,dtype=torch.float)\n",
    "t = torch.normal(mean,std)\n",
    "print(\"mean:{},std:{}\".format(mean,std))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1192, -0.1618,  1.2117, -0.4006,  0.6756])\n"
     ]
    }
   ],
   "source": [
    "#mean为标量，std为标量\n",
    "t = torch.normal(0.2,1.0,size=(5,))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:2,std:tensor([1., 2., 3.])\n",
      "tensor([ 2.5615,  2.2416, -1.5931])\n"
     ]
    }
   ],
   "source": [
    "#mean为标量，std为张量\n",
    "mean = 2\n",
    "std = torch.arange(1,4,dtype=torch.float)\n",
    "t = torch.normal(mean,std)\n",
    "print(\"mean:{},std:{}\".format(mean,std))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:tensor([1., 2., 3.]),std:2\n",
      "tensor([-0.9332,  3.0824,  1.1787])\n"
     ]
    }
   ],
   "source": [
    "#mean为张量，std为标量\n",
    "mean = torch.arange(1,4,dtype=torch.float)\n",
    "std = 2\n",
    "t = torch.normal(mean,std)\n",
    "print(\"mean:{},std:{}\".format(mean,std))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准正态分布数据的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6828,  0.3659, -0.3481,  0.2043, -1.3155, -0.5559])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3571e-01, -2.4681e-01, -7.6430e-01, -1.5986e+00],\n",
      "        [ 8.1159e-04, -2.8634e-01, -3.8962e-01,  1.1206e+00],\n",
      "        [-1.5792e+00,  1.0334e+00, -1.2379e+00,  1.3063e+00]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((3,4))\n",
    "t = torch.randn_like(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3216, 0.4315, 0.6821, 0.8690],\n",
       "        [0.2943, 0.2675, 0.7112, 0.8871],\n",
       "        [0.3668, 0.3642, 0.5212, 0.2390]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 5, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(2, 6, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 2],\n",
       "        [7, 2],\n",
       "        [8, 7]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(9, (3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 1, 5, 4, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8743, 0.3144, 0.0818],\n",
       "        [0.0236, 0.0744, 0.3838],\n",
       "        [0.5188, 0.8553, 0.4509]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(3, 3).uniform_(0, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(a)#使用上面创建的张量a作为概率值创建伯努利分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((3,2))\n",
    "t0 = torch.cat([t,t],dim=0)#在第0个维度上拼接\n",
    "t1 = torch.cat([t,t],dim=1)#在第1个维度上拼接\n",
    "print(t0,'\\n\\n',t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.cat([t,t,t],dim=0)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((3,2))\n",
    "t1 = torch.stack([t,t],dim=2)#在新创建的维度上进行拼接\n",
    "print(t1,t1.shape) #拼接完会从2维变成3维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]]) torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones((3,2))\n",
    "t1 = torch.stack([t,t],dim=0)#在新创建的维度上进行拼接\n",
    "#由于指定是第0维，会把原来的3，2往后移动一格，然后在新的第0维创建新维度进行拼接\n",
    "print(t1,t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  使用torch.chunk()切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]) torch.Size([3, 2])\n",
      "1 tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((5,2))\n",
    "t = torch.chunk(a,dim=0,chunks=2)#在5这个维度切分，切分成2个张量\n",
    "for idx, t_chunk in enumerate(t):\n",
    "    print(idx,t_chunk,t_chunk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用torch.split()切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2])\n",
      "1 tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2])\n",
      "2 tensor([[1., 1.]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((5,2))\n",
    "t = torch.split(a,2,dim=0)#指定了每个张量的长度为2\n",
    "for idx, t_split in enumerate(t):\n",
    "    print(idx,t_split,t_split.shape)#切出3个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2])\n",
      "1 tensor([[1., 1.]]) torch.Size([1, 2])\n",
      "2 tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((5,2))\n",
    "t = torch.split(a,[2,1,2],dim=0)#指定了每个张量的长度为列表中的大小【2，1，2】\n",
    "for idx, t_split in enumerate(t):\n",
    "    print(idx,t_split,t_split.shape)#切出3个张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  2,  5],\n",
      "        [ 5,  3,  6],\n",
      "        [ 4, 11,  6],\n",
      "        [ 8,  7,  5]]) \n",
      " tensor([[False, False, False],\n",
      "        [False, False,  True],\n",
      "        [False,  True,  True],\n",
      "        [ True,  True, False]]) \n",
      " tensor([ 6, 11,  6,  8,  7])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0,12,size=(4,3))\n",
    "mask = t.ge(6) #ge是greater than or equal ,gt是greater than , le   lt \n",
    "t_select = torch.masked_select(t,mask)\n",
    "print(t,'\\n',mask,'\\n',t_select)#将大于等于6的数据挑出来，返回一维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 8, 0, 1, 9, 2, 6, 5, 7, 4]) \n",
      " tensor([[3, 8, 0, 1, 9],\n",
      "        [2, 6, 5, 7, 4]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(10)\n",
    "t1 = torch.reshape(t,(2,5))\n",
    "print(t,'\\n',t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 8, 0, 1, 9, 2, 6, 5, 7, 4]) \n",
      " tensor([[3, 8, 0, 1, 9],\n",
      "        [2, 6, 5, 7, 4]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.reshape(t,(-1,5))# -1代表根据其他维度计算得到\n",
    "print(t,'\\n',t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1024,    1,    5,    2,    6,    8,    9,    7,    0,    3]) \n",
      " tensor([[3, 8, 0, 1, 9],\n",
      "        [2, 6, 5, 7, 4]])\n",
      "2084622541192 2084622541192\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(10)\n",
    "t[0] = 1024\n",
    "print(t,'\\n',t1)\n",
    "print(id(t.data),id(t1.data))\n",
    "#共享内存，id相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2]) torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((4,3,2))\n",
    "t1 = torch.transpose(t,dim0=0,dim1=1)#交换他们的第0，1维度\n",
    "print(t.shape,t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4659,  2.2305],\n",
      "        [ 0.0881,  1.6183],\n",
      "        [ 0.7413, -0.6628]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4659,  0.0881,  0.7413],\n",
       "        [ 2.2305,  1.6183, -0.6628]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "print(x)\n",
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1, 1]) \n",
      " torch.Size([2]) torch.Size([2, 1, 1]) torch.Size([1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((1,2,1,1))\n",
    "t1 = torch.squeeze(t)\n",
    "t2 = torch.squeeze(t,dim=0)\n",
    "t3 = torch.squeeze(t,dim=1)#指定的轴长度不为1，不能移除\n",
    "print(t.shape,'\\n',t1.shape,t2.shape,t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "torch.unsqueeze(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
