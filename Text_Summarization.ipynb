{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10061]\n",
      "[nltk_data]     由于目标计算机积极拒绝，无法连接。>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text=\"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "clean_text = text.lower()\n",
    "clean_text = re.sub(r'\\W',' ',clean_text)\n",
    "clean_text = re.sub(r'\\d',' ',clean_text)\n",
    "clean_text = re.sub(r'\\s+',' ',clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Stopword list\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 1.0,\n",
       " 'much': 0.25,\n",
       " 'academy': 0.125,\n",
       " 'room': 0.125,\n",
       " 'congratulate': 0.125,\n",
       " 'incredible': 0.125,\n",
       " 'nominees': 0.125,\n",
       " 'year': 0.25,\n",
       " 'revenant': 0.25,\n",
       " 'product': 0.125,\n",
       " 'tireless': 0.125,\n",
       " 'efforts': 0.125,\n",
       " 'unbelievable': 0.125,\n",
       " 'cast': 0.125,\n",
       " 'crew': 0.125,\n",
       " 'first': 0.125,\n",
       " 'brother': 0.125,\n",
       " 'endeavor': 0.125,\n",
       " 'mr': 0.125,\n",
       " 'tom': 0.25,\n",
       " 'hardy': 0.125,\n",
       " 'talent': 0.125,\n",
       " 'screen': 0.25,\n",
       " 'surpassed': 0.125,\n",
       " 'friendship': 0.125,\n",
       " 'creating': 0.125,\n",
       " 'ranscendent': 0.125,\n",
       " 'cinematic': 0.125,\n",
       " 'experience': 0.125,\n",
       " 'everybody': 0.125,\n",
       " 'fox': 0.125,\n",
       " 'new': 0.125,\n",
       " 'regency': 0.125,\n",
       " 'entire': 0.25,\n",
       " 'team': 0.125,\n",
       " 'everyone': 0.125,\n",
       " 'onset': 0.125,\n",
       " 'career': 0.125,\n",
       " 'parents': 0.125,\n",
       " 'none': 0.125,\n",
       " 'would': 0.25,\n",
       " 'possible': 0.125,\n",
       " 'without': 0.125,\n",
       " 'friends': 0.125,\n",
       " 'love': 0.125,\n",
       " 'dearly': 0.125,\n",
       " 'know': 0.125,\n",
       " 'lastly': 0.125,\n",
       " 'want': 0.125,\n",
       " 'say': 0.125,\n",
       " 'making': 0.125,\n",
       " 'man': 0.125,\n",
       " 'relationship': 0.125,\n",
       " 'natural': 0.125,\n",
       " 'world': 0.5,\n",
       " 'collectively': 0.25,\n",
       " 'felt': 0.125,\n",
       " 'hottest': 0.125,\n",
       " 'recorded': 0.125,\n",
       " 'history': 0.125,\n",
       " 'production': 0.125,\n",
       " 'needed': 0.125,\n",
       " 'move': 0.125,\n",
       " 'southern': 0.125,\n",
       " 'tip': 0.125,\n",
       " 'planet': 0.25,\n",
       " 'able': 0.125,\n",
       " 'find': 0.125,\n",
       " 'snow': 0.125,\n",
       " 'climate': 0.125,\n",
       " 'change': 0.125,\n",
       " 'real': 0.125,\n",
       " 'happening': 0.125,\n",
       " 'right': 0.125,\n",
       " 'urgent': 0.125,\n",
       " 'threat': 0.125,\n",
       " 'facing': 0.125,\n",
       " 'species': 0.125,\n",
       " 'need': 0.25,\n",
       " 'work': 0.125,\n",
       " 'together': 0.125,\n",
       " 'stop': 0.125,\n",
       " 'procrastinating': 0.125,\n",
       " 'support': 0.125,\n",
       " 'leaders': 0.125,\n",
       " 'around': 0.125,\n",
       " 'speak': 0.25,\n",
       " 'big': 0.125,\n",
       " 'polluters': 0.125,\n",
       " 'humanity': 0.125,\n",
       " 'indigenous': 0.125,\n",
       " 'people': 0.375,\n",
       " 'billions': 0.25,\n",
       " 'underprivileged': 0.125,\n",
       " 'affected': 0.125,\n",
       " 'children': 0.25,\n",
       " 'whose': 0.125,\n",
       " 'voices': 0.125,\n",
       " 'drowned': 0.125,\n",
       " 'politics': 0.125,\n",
       " 'greed': 0.125,\n",
       " 'amazing': 0.125,\n",
       " 'award': 0.125,\n",
       " 'tonight': 0.25,\n",
       " 'let': 0.125,\n",
       " 'us': 0.125,\n",
       " 'take': 0.25,\n",
       " 'granted': 0.25}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts \n",
    "word2count = {}\n",
    "for word in nltk.word_tokenize(clean_text):\n",
    "    if word not in stop_words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "# Converting counts to weights\n",
    "max_count = max(word2count.values())\n",
    "for key in word2count.keys():\n",
    "    word2count[key] = word2count[key]/max_count\n",
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thank you all so very much.': 1.25,\n",
       " 'Thank you to the Academy.': 1.125,\n",
       " 'Thank you to all of you in this room.': 1.125,\n",
       " 'I have to congratulate the other incredible nominees this year.': 0.625,\n",
       " 'The Revenant was the product of the tireless efforts of an unbelievable cast and crew.': 1.0,\n",
       " 'First off, to my brother in this endeavor, Mr. Tom Hardy.': 0.75,\n",
       " 'Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience.': 2.625,\n",
       " 'Thank you to everybody at Fox and New Regency … my entire team.': 1.875,\n",
       " 'I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you.': 2.125,\n",
       " 'And to my friends, I love you dearly; you know who you are.': 0.5,\n",
       " \"And lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world.\": 1.625,\n",
       " 'A world that we collectively felt in 2015 as the hottest year in recorded history.': 1.5,\n",
       " 'Our production needed to move to the southern tip of this planet just to be able to find snow.': 1.25,\n",
       " 'Climate change is real, it is happening right now.': 0.625,\n",
       " 'It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating.': 1.75,\n",
       " 'For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed.': 1.5,\n",
       " 'I thank you all for this amazing award tonight.': 1.5,\n",
       " 'Let us not take this planet for granted.': 1.0,\n",
       " 'I do not take tonight for granted.': 0.75,\n",
       " 'Thank you so very much.': 1.25}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product sentence scores    \n",
    "sent2score = {}\n",
    "for sentence in sentences:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word2count.keys():\n",
    "            if len(sentence.split(' ')) < 25:\n",
    "                if sentence not in sent2score.keys():\n",
    "                    sent2score[sentence] = word2count[word]\n",
    "                else:\n",
    "                    sent2score[sentence] += word2count[word]\n",
    "sent2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience.\n",
      "I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you.\n",
      "Thank you to everybody at Fox and New Regency … my entire team.\n",
      "It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating.\n",
      "And lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "# Gettings best 5 lines             \n",
    "best_sentences = heapq.nlargest(5, sent2score, key=sent2score.get)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "for sentence in best_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
